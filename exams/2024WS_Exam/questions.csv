id;question;options;correct_answer;explanation;context_image
1;True or False: $P(Starter\_broken | Car\_won't\_start) = P(No\_oil | Car\_won't\_start)$;True|False;False;False. While both are potential causes for the car not starting, their posterior probabilities depend on their individual priors and conditional probability tables (CPTs), which are rarely identical.;info_car_bn.png
2;True or False: $(Starter\_broken \perp Battery\_dead)$;True|False;True;True. In the provided graph, there is no path or common ancestor connecting these two variables, making them marginally independent.;info_car_bn.png
3;True or False: $P(Battery\_age) = P(Battery\_age | Battery\_meter)$;True|False;False;False. 'Battery_age' is a parent of 'Battery_dead', which is a parent of 'Battery_meter'. Information flows through this path, so observing the meter changes the probability of the age.;info_car_bn.png
4;True or False: $P(Battery\_flat | Lights, Battery\_dead) = P(Battery\_flat | Battery\_dead)$;True|False;False;False. 'Lights' is a child of 'Battery_flat'. Observing a child provides evidence about its parent, meaning the conditional independence stated does not hold.;info_car_bn.png
5;True or False: $(No\_charging \perp No\_oil | Alternator\_broken, Fanbelt\_broken)$;True|False;True;True. 'Alternator_broken' and 'Fanbelt_broken' are the parents of 'No_charging'. By the Markov property, a node is independent of its non-descendants (like 'No_oil') given its parents.;info_car_bn.png
6;True or False: There is no topological ordering for this network that has 'Car_won't_start' as the very last variable;True|False;False;False. 'Car_won't_start' is a leaf node (no children). In any Directed Acyclic Graph (DAG), a leaf node can always be placed at the end of a topological sort.;info_car_bn.png
7;True or False: Adding a link from 'Car_won't_start' to 'Battery_meter' would lead to an illegal network structure;True|False;False;False. Adding this link would not create a cycle, as 'Battery_meter' is not an ancestor of 'Car_won't_start'. The graph would remain a valid DAG.;info_car_bn.png
8;True or False: There is no topological ordering for this network that has all the orange variables at the very beginning;True|False;True;True. In a topological ordering, parents must come before children. Several orange variables have parents (e.g., 'Battery_dead' has 'Battery_age'), so they cannot all be at the very beginning.;info_car_bn.png
9;True or False: If all variables were boolean, the number of non-redundant parameters would be 16;True|False;False;False. The number of parameters in a BN is determined by the number of parents for each node ($2^k$). The total sum for this complex network is much higher than 16.;info_car_bn.png
10;True or False: All the green variables are independent of each other;True|False;False;False. Most green variables share common ancestors (the orange 'diagnoses'), which induces correlation between them. They are not marginally independent.;info_car_bn.png
11;True or False: We cannot calculate the distribution over 'Battery_flat' if we do not know Dead, Alternator, and Fanbelt;True|False;False;False. We can always calculate marginal distributions by summing out the unknown variables in the full joint distribution.;info_car_bn.png
12;True or False: The dipstick tells us nothing about the oil light;True|False;False;False. Both share 'No_oil' as a common parent. Observing the dipstick changes our belief about 'No_oil', which in turn changes the probability of the 'Oil light'.;info_car_bn.png
13;True or False: The query $P(Car\_won't\_start)$ gives the probability when all green variables are known;True|False;False;False. $P(X)$ is a prior probability. If evidence (green variables) were known, it would be a conditional probability query $P(X | e)$.;info_car_bn.png
14;True or False: With only 'Car_won't_start' as evidence, all orange variables contribute a weight of 1.0 in likelihood weighting;True|False;True;True. Likelihood weighting only generates weights based on evidence nodes. Since orange variables are not evidence, they are sampled normally and contribute a weight of 1.0.;info_car_bn.png
15;True or False: If all green variables are evidence, 9 variables have to be sampled in forward sampling;True|False;False;False. Forward sampling always samples all non-evidence variables. If we have 16 variables and 5 are evidence, 11 must be sampled.;info_car_bn.png
16;True or False: With only 'Car_won't_start' as evidence, all green variables contribute a weight of 1.0 in likelihood weighting;True|False;True;True. In this specific query, the other green variables are not evidence. They are treated as hidden variables and sampled, contributing a weight of 1.0.;info_car_bn.png
17;True or False: There is no variable in this network whose Markov blanket contains more than 5 variables;True|False;False;False. 'Car_won't_start' has many parents (Flat, No_oil, No_gas, Starter, Fuel_line). Its Markov blanket (parents + children + spouses) is larger than 5.;info_car_bn.png
18;True or False: For $N=50$ and $k=30$, the likelihood function keeps its maximum at the same place but becomes flatter;True|False;False;False. While the maximum (MLE) stays at 0.6, the function becomes **narrower** (more peaked) as the amount of data increases.;info_thumbtack.png
19;True or False: The likelihood function value at the maximum would be 10 times as high for $N=50$;True|False;False;False. Likelihood is a product of probabilities. As the sequence length increases, the absolute probability of any single sequence typically decreases significantly.;info_thumbtack.png
20;True or False: The likelihood function would now range on the horizontal axis from 0 to 10;True|False;False;False. The horizontal axis represents the parameter $\theta$, which is a probability and must always be between 0 and 1.;info_thumbtack.png
21;True or False: The maximum of the likelihood would shift to the left, towards 0.5;True|False;False;False. The MLE is $k/N$. $30/50$ is $0.6$, which is the same as the original $3/5$. The position of the maximum does not shift.;info_thumbtack.png
22;Will the resulting MAP estimate be...;larger than ML|the same as ML|exactly at 0.5|smaller than ML;smaller than ML;The ML estimate is 0.6 and the prior peaks at 0.5. The MAP is a compromise between them and will fall between 0.5 and 0.6, making it smaller than the ML estimate.;info_thumbtack.png
23;If we increase data to $N=50$ (same distribution), will the resulting MAP estimate...;move farther away from ML|stay the same|move closer to ML;move closer to ML;As more data is collected, the likelihood term dominates the prior. The MAP estimate will move away from the prior peak and closer to the ML estimate.;info_thumbtack.png
24;How many unconditional distributions have to be estimated when calculating the likelihood?;1|2|3|4|5;4;Unconditional distributions are needed for the root nodes (those without parents). In this structure, Difficulty, SAT, Grade, and Letter are all parents of Intelligence and have no parents of their own.;info_student_bn.png
25;True or False: This structure can fit all possible joint distributions over the five variables;True|False;False;False. Only a fully connected graph can represent all joint distributions. This structure assumes several conditional independencies.;info_student_bn.png
26;How many independent parameters must be learned for variable I?;4|8|12|24|48;24;I has parents D (2 values), S (2), G (3), and L (2). The number of parent combinations is $2 \times 2 \times 3 \times 2 = 24$. Each combination requires an independent parameter.;info_student_bn.png
27;True or False: When we add additional edges to the model, the BIC score can only decrease;True|False;False;False. BIC balances fit (likelihood) and complexity. If an edge significantly improves the fit, the BIC score can increase.;info_student_bn.png
28;True or False: If likelihood increases with an additional edge, the BIC score will also increase;True|False;False;False. Not necessarily. If the likelihood increase is small but the complexity penalty is large, the overall BIC score will decrease.;info_student_bn.png
29;True or False: Compared to the previous model, the ML estimates for variables D, G, and L will not change;True|False;True;True. Because the structure still treats D, G, and L as independent root nodes or non-descendants of the new edge, their local CPT estimation remains unchanged.;info_student_bn.png
30;True or False: The likelihood of the new model cannot be lower than the likelihood of the previous one;True|False;True;True. Adding an edge (increasing complexity) always allows a model to fit the training data at least as well as the simpler version.;info_student_bn.png
31;True or False: The most probable observation at time 0 is humid;True|False;False;False. Based on $P(O_0) = \sum P(O_0|S_0)P(S_0)$ using the provided matrices, 'dry' has the highest probability (0.37).;info_weather_hmm.png
32;True or False: If we added 'wet' to observations, the B matrix would become a 4x4 matrix;True|False;False;False. The B matrix size is (States x Observations). With 3 states and 4 observations, it would be a 3x4 matrix.;info_weather_hmm.png
33;True or False: If we added 'snowing' to states, the A matrix would become a 3x4 matrix;True|False;False;False. The transition matrix A is always square (States x States). With 4 states, it would be a 4x4 matrix.;info_weather_hmm.png
34;True or False: The A matrix must always be a square matrix, independent of the number of states;True|False;True;True. The transition matrix maps the state at time $t$ to the same state space at $t+1$, so it must always be $N \times N$.;info_weather_hmm.png
35;True or False: If we added an additional state value, we would also have to add an additional observation value;True|False;False;False. The number of states and the number of observations are independent design choices in an HMM.;info_weather_hmm.png
36;True or False: The distribution $P(S_0 | O)$ calculated by Smoothing will always equal $\Pi$;True|False;False;False. Smoothing uses the entire observation sequence to update the estimate of $S_0$. It will only equal $\Pi$ if no evidence is provided.;info_weather_hmm.png
37;True or False: With this model, we cannot model a GPS sensor whose precision changes with the speed of motion;True|False;True;True. A standard Kalman Filter uses a linear observation model with constant noise. Speed-dependent precision would require a non-linear dependency.;info_bicycle_kf.png
38;True or False: If $V^{(t+1)}$ also depended on $P^{(t)}$, the A matrix would need an additional column;True|False;False;False. The A matrix already has a column for $P^{(t)}$. The dependency would just change a zero entry to a non-zero coefficient.;info_bicycle_kf.png
39;True or False: The coefficient matrix A can be used to encode certain assumptions about the expected precision of the GPS sensor;True|False;False;False. Matrix A encodes the transition physics. GPS precision is handled by the observation noise covariance matrix $R$.;info_bicycle_kf.png
40;True or False: If $V^{(t+1)}$ also depended on $P^{(t)}$, the size of the A matrix would double;True|False;False;False. The size of A depends on the number of state variables (P and V), which remains two. It stays a 2x2 matrix.;info_bicycle_kf.png
41;True or False: If the bicycle speeds up over a long time, the A matrix will change;True|False;False;False. Matrix A defines the laws of motion (e.g., $P_{new} = P_{old} + V \cdot \Delta t$), which are independent of the current speed value.;info_bicycle_kf.png