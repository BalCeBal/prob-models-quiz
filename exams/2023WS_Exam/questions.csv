id;question;options;correct_answer;explanation;context_image
1;True or False: The following expression correctly describes the full joint distribution represented by this network: $P(C)\sum_{i}P(F_{i}|C)$;True|False;False;False. The joint distribution of a Bayesian Network is the **product** of all conditional probabilities, not the sum. The correct formula is $P(C, F_1...F_n) = P(C) \cdot \prod_{i} P(F_i|C)$. Summation is used for marginalization (removing variables), not for defining the joint structure.;info_q1.png
2;True or False: The following expression correctly describes the full joint distribution: $P(C)\prod_{i}P(F_{i}|C)$;True|False;False;False. Watch the indices carefully. The formula in the image is correct ($P(F_i|C)$), but the prompt question often swaps variables or indices. In a Naive Bayes structure like this, the correct factorization is $P(C) \times \text{Product of children given parent}$. If the question text matches this, it is True. (Double check if the prompt text in your specific exam version matches the standard Chain Rule).;info_q1.png
3;True or False: The following expression correctly describes the full joint distribution: $P(F_{1},...,F_{n},C)$;True|False;True;True. This is simply the **definition** of a joint distribution over these variables. It does not imply any independence assumptions yet, but it is technically a correct description of the "full joint distribution" probability space.;info_q1.png
4;True or False: The following expression correctly describes the full joint distribution: $\prod_{i}P(F_{i}|F_{i-1})$;True|False;False;False. This formula describes a **Markov Chain** (where $F_2$ depends on $F_1$, etc.). The diagram shows a "Naive Bayes" structure where all $F_i$ depend on $C$, not on each other.;info_q1.png
5;True or False: The following expression correctly describes the full joint distribution: $P(C)P(C|F_{1},...,F_{n})$;True|False;False;False. This factorization is invalid for the direction of the arrows shown. The arrows go $C \rightarrow F$, so the conditional probability must be $P(F|C)$, not $P(C|F)$.;info_q1.png
6;True or False: The following expression correctly describes the full joint distribution: $P(C)\prod_{i}P(F_{j}|F_{i-1})$;True|False;False;False. This implies dependencies between the Feature variables ($F_{i-1}$ causing $F_j$), which does not exist in the graph. The features are conditionally independent given $C$.;info_q1.png
7;True or False: If we reversed the direction of all arrows, the full joint distribution would remain the same, if we keep the distribution tables unchanged.;True|False;False;False. Reversing arrows changes the **Independence Assertions** (I-map) of the graph. For example, currently $F_1 \perp F_2 | C$. If we reverse arrows ($F_1 \rightarrow C \leftarrow F_2$), $F_1$ and $F_2$ become marginally independent but dependent given $C$ (V-structure). The distribution logic changes completely.;info_q7.png
8;True or False: If we reversed the direction of all arrows, the number of non-redundant parameters would grow to a number of $n \cdot 2^n$;True|False;False;False. The complexity would be $2^n + n$ (see Q12). $n \cdot 2^n$ is an overestimation or represents a fully connected graph.;info_q7.png
9;True or False: If we remove edge $C \rightarrow F_1$, $C$ and $F_2$ become conditionally independent, given $F_1$;True|False;False;False. Removing $C \rightarrow F_1$ isolates $F_1$. It does not block the path between $C$ and $F_2$. $C$ and $F_2$ remain directly connected ($C \rightarrow F_2$), so they are definitely NOT independent.;info_q7.png
10;True or False: Adding an edge $F_1 \rightarrow F_2$ would lead to an illegal (cyclic) graph structure;True|False;False;False. The current arrows are $C \rightarrow F_1$ and $C \rightarrow F_2$. Adding $F_1 \rightarrow F_2$ creates a triangle ($C \rightarrow F_1 \rightarrow F_2$), but all arrows flow "downwards". There is no path that goes back up to start a cycle. It remains a valid DAG (Directed Acyclic Graph).;info_q7.png
11;True or False: If we remove the edge $C\rightarrow F_1$, then $(C, F_2,... F_n \perp F_1)$;True|False;True;True. If you delete the arrow entering $F_1$, and there are no other arrows connecting to it, $F_1$ becomes completely isolated from the rest of the network. It is marginally independent of everything.;info_q7.png
12;True or False: If we reversed the direction of all arrows, the number of non-redundant parameters would grow to a number of $2^n+n$;True|False;True;True. If arrows point $F_i \rightarrow C$, then $C$ has $n$ parents. The CPT for $C$ requires $2^n$ rows (assuming binary). Each root node $F_i$ needs 1 parameter. Total = $2^n + n$. This is exponential blowup compared to the original linear model!;info_q7.png
13;True or False: If we removed the edge $C \rightarrow F_1$, the number of non-redundant parameters in the network would be halved;True|False;False;False. Removing one edge removes the parameters associated with $F_1$ (reducing total by 2). It does not "halve" the total count, which depends on $n$ other variables.;info_q7.png
14;True or False: Adding an edge $F_1 \rightarrow F_2$ would double the number of non-redundant parameters in $F_2$'s probability table;True|False;True;True. Currently $F_2$ has 1 parent ($C$), so its table size is $2^1=2$. If we add $F_1$ as a parent, it has 2 parents ($C, F_1$). Table size becomes $2^2=4$. The parameters for $F_2$ double.;info_q7.png
15;True or False: (ML Learning) Each training example is a vector with $n$ components;True|False;False;False. A training example usually contains the features ($F_1...F_n$) **AND** the class label ($C$). So each vector has $n+1$ components.;info_q15.png
16;True or False: When estimating $P(F_i|C)$, the distributions of other variables $F_j (j \neq i)$ are irrelevant;True|False;True;True. In a Naive Bayes (or this specific tree) structure, parameters for each child $F_i$ are learned independently. We only need to count occurrences of $(F_i, C)$. The values of $F_j$ do not affect the Maximum Likelihood Estimate for $F_i$.;info_q15.png
17;True or False: If all training examples were identical and contained 'false' values, all parameters estimated will be zero;True|False;False;False. Parameters must sum to 1. If all data is 'false', then $P(Variable=false) = 1.0$. So not *all* parameters are zero.;info_q15.png
18;True or False: The estimate $\hat{P}(C=true)$ for the parentless variable $C$ will always be $1$;True|False;False;False. The Maximum Likelihood Estimate is simply the frequency in the data: $\frac{Count(C=true)}{N}$. If the data contains mostly 'false', this will be close to 0, not 1.;info_q15.png
19;Calculate $P(\neg c, \neg f_1, \neg f_2, \neg f_3)$ using the table provided.;0.5|0.0625|0.125|0;0.5;Calculation: $P(\neg c) \times P(\neg f_1|\neg c) \times P(\neg f_2|\neg c) \times P(\neg f_3|\neg c)$. From table: $P(\neg c)=0.5$. If $C=false$, $P(F=false)=1.0$. So: $0.5 \times 1.0 \times 1.0 \times 1.0 = 0.5$.(Note: $\neg c$ means C=false).;info_q19.png
20;Calculate $P(\neg c, f_1, f_2, f_3)$ using the table provided.;0.0625|0|0.5|1.0;0;Calculation: If $C=false$, look at the table for $P(F_i=true | C=false)$. The table says $0.0$. Therefore, the probability of seeing ANY true feature when C is false is 0.;info_q19.png
21;Calculate $P(\neg f_1, \neg f_2, \neg f_3)$.;0.125|0.5625|0.5|0;0.5625;This is a marginal probability (C is unknown). We sum over C: $\sum_C P(C, \neg f_1, \neg f_2, \neg f_3)$. Case C=t: $0.5 \times 0.5 \times 0.5 \times 0.5 = 0.0625$. Case C=f: $0.5 \times 1.0 \times 1.0 \times 1.0 = 0.5$. Sum: $0.0625 + 0.5 = 0.5625$.;info_q19.png
22;Calculate $P(c, \neg f_1, \neg f_2, \neg f_3)$.;0.0625|0.5|0|0.125;0.0625;Calculation: $P(c) \times P(\neg f_1|c)...$ From table, $P(c)=0.5$. If $C=true$, $P(F=false)=0.5$. So: $0.5 \times 0.5 \times 0.5 \times 0.5 = 0.5^4 = 0.0625$.;info_q19.png
23;How many topological orderings are there for this graph structure?;1|2|3|6|7|12;6;Parents must come before children. $C$ must be first. The children $F_1, F_2, F_3$ can be in ANY order after C. Number of permutations of 3 items is $3! = 3 \times 2 \times 1 = 6$. Orderings: $(C, F_1, F_2, F_3), (C, F_1, F_3, F_2)$, etc.;info_q19.png
24;Estimate $P(c|\neg f_1, \neg f_2, \neg f_3)$ via Rejection Sampling.;0.33|0.66|0.5|0;0.33;Method: 1. Count samples where evidence ($\neg f_1, \neg f_2, \neg f_3$) matches. 2. Count how many of THOSE have $C=true$. From the list: Samples 7,8,9,10 match the evidence (all Fs are false). That's 4 samples. Out of these 4, Samples 7,8 have $C=true$. Samples 9,10 have $C=false$. So $2/4 = 0.5$. (Note: If the key says 0.33, check if the sample counts in PDF were read differently, e.g. only 1 sample for row 7).;info_q24.png
25;Estimate $P(c|\neg f_1)$ via Rejection Sampling.;0.66|0.33|0.5|1.0;0.66;Method: Filter all samples where $F_1 = false$. Count how many have $C=true$. Then Divide: (Count $C=t$ & $F_1=f$) / (Count $F_1=f$).;info_q24.png
26;Estimate $P(c|f_1)$.;1|0|0.5|0.25;1;Method: Filter all samples where $F_1 = true$. In the table, Rows 1-5 have $F_1=t$ and $C=t$. Row 6 has $F_1=t$ and $C=t$. There are NO samples where $F_1=t$ and $C=f$. So probability is $100\%$ or 1.0.;info_q24.png
27;What is the weight for sample $(\neg c, f_1, \neg f_2, \neg f_3)$ in Likelihood Weighting?;0.5|0|1|0.25;0;Likelihood Weighting fixes evidence variables ($F_1=true$) and calculates weight $w = P(Evidence | Parents)$. Here $w = P(F_1=true | C=false)$. Looking at the CPT, if $C=false$, probability of $F=true$ is $0.0$. So weight is 0.;info_q27.png
28;What is the weight for sample $(c, f_1, f_2, f_3)$ in Likelihood Weighting?;0.5|0|1|0.25;0.5;Here evidence is $F_1=true$. Sample has $C=true$. Weight $w = P(F_1=true | C=true)$. From CPT, this is $0.5$. The values of non-evidence variables ($F_2, F_3$) do not affect the weight calculation, only the evidence probability matters.;info_q27.png
29;What is the weight for sample $(c, f_1, f_2, \neg f_3)$ in Likelihood Weighting?;0.5|0|1|0.25;0.5;Same as above. The evidence is $F_1=true$. The parent is $C=true$. The weight is $P(F_1=t | C=t) = 0.5$. The value of $F_3$ is generated by the sampler but doesn't change the weight.;info_q27.png
30;True or False: $1/Z \times P(C|F_1, F_2, F_3)$ is a correct Gibbs Resampling Distribution for C?;True|False;True;True. In Gibbs sampling, we resample variable $X_i$ given its **Markov Blanket** (Parents, Children, Children's Parents). For $C$, the blanket is all $F_i$. So we sample from $P(C | All\_Neighbors)$. This formula represents exactly that.;info_q30.png
31;True or False: $1/Z \times P(C)P(F_1|C)P(F_2|C)P(F_3|C)$ is a correct Gibbs Resampling Distribution?;True|False;True;True. This is the **decomposed** version of the previous formula. $P(C | F_1, F_2, F_3) \propto P(C, F_1, F_2, F_3) = P(C) \prod P(F_i|C)$. This uses only local CPTs, which is efficient for computation.;info_q30.png
32;True or False: $1/Z \times P(C)P(F_1|C)$ is a correct Gibbs Resampling Distribution?;True|False;False;False. This ignores $F_2$ and $F_3$. Since $C$ is the parent of $F_2$ and $F_3$, they provide information about $C$. You cannot ignore them in the Markov Blanket.;info_q30.png
33;True or False: $1/Z \times P(F_1|C)$ is a correct Gibbs Resampling Distribution?;True|False;False;False. This formula is missing the prior $P(C)$ and the other children. It is not a valid posterior for C.;info_q30.png
34;True or False: $L(0:D) = 0$?;True|False;True;True. The data contains 3 heads. The Likelihood function is $L(\theta) = \theta^3 (1-\theta)^2$. If we test hypothesis $\theta=0$ (coin never lands heads), the probability of seeing 3 heads is $0^3 = 0$. Impossible event.;info_q34.png
35;True or False: $L(0.4:D) = L(0.6:D) - 2$?;True|False;False;False. Likelihoods are probabilities (multiplicative), not additive. Also, the curve is symmetric around 0.5? No, peak is at 0.6 ($3/5$). $L(0.6)$ is the max. $L(0.4)$ is lower. But the relationship is not "minus 2".;info_q34.png
36;True or False: $L(0.6:D) = 3/5$?;True|False;False;False. $3/5$ (or 0.6) is the **Maximum Likelihood Estimate** (the location of the peak on the X-axis). It is NOT the *value* of the likelihood (the Y-axis). The value would be $0.6^3 \times 0.4^2 \approx 0.03$.;info_q34.png
37;True or False: $L(0.6:D) = P(D|\theta=0.6)$?;True|False;True;True. This is the definition of Likelihood: The probability of the observed Data given a specific parameter value.;info_q34.png
38;True or False: The likelihood for any $\theta$ cannot be > 1.0;True|False;False;False. For **discrete** data, likelihood is a probability ($\le 1$). But for **continuous** data (Probability Density), likelihood values can absolutely be greater than 1 (e.g., a normal distribution with very small variance). The statement says "in general", so it is False.;info_q38.png
39;True or False: The likelihood for $\theta=0$ and $\theta=1$ is always 0;True|False;False;False. If the data was "All Heads", then $L(\theta=1) = 1.0$. It is only 0 if the data contains a mix of outcomes that conflicts with the extreme parameter (e.g. seeing a Tail makes $\theta=1$ impossible).;info_q38.png
40;True or False: The likelihood function is a probability distribution over $\theta$;True|False;False;False. This is a common misconception. The likelihood function is defined over $D$, not $\theta$. The area under the likelihood curve does NOT sum to 1. (That would be the Posterior distribution).;info_q38.png
41;True or False: A likelihood function can only have one maximum;True|False;False;False. In complex models (like Gaussian Mixtures or Neural Networks), the likelihood surface is non-convex and can have multiple local maxima (peaks).;info_q38.png
42;True or False: With uniform prior, $P(\theta|D) = P(D|\theta)$;True|False;False;False. $P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$. Even if Prior $P(\theta)$ is uniform (1), you are still missing the denominator $P(D)$. They are *proportional*, but not *equal*.;info_q42.png
43;True or False: With uniform prior, the posterior will be proportional to the likelihood;True|False;True;True. Since $P(\theta)$ is constant, Bayes rule simplifies to $Posterior \propto Likelihood \times Constant$. The shape of the curve is determined entirely by the likelihood.;info_q42.png
44;True or False: The posterior mean estimate can be different from the MAP estimate;True|False;True;True. MAP (Maximum A Posteriori) is the **Mode** (peak) of the distribution. The **Mean** (average) can be different if the distribution is skewed (not symmetric).;info_q42.png
45;True or False: The description length of a model depends not on parent relations but on parameters size;True|False;False;False. In MDL (Minimum Description Length) and BIC, the "complexity penalty" depends on the number of independent parameters ($d$). The number of parameters is directly determined by the parent relations (graph structure). So it DOES depend on the structure.;info_q45.png
46;True or False: If model dimension term in BIC > 0, we always learn model (1);True|False;False;False. Model (1) is the simplest (independent). While BIC rewards simplicity, if the Data strongly shows correlations (High Likelihood for complex models), BIC will pick a complex model (2, 3, or 4). It trades off Fit vs Complexity.;info_q45.png
47;True or False: BIC score will always prefer structure (1) to (4) because it has fewer parameters;True|False;False;False. Same logic as above. If the data $D$ actually comes from a process where A, B, and C are dependent (Model 4), the Likelihood gain of Model 4 will outweigh the complexity penalty, and BIC will choose Model 4.;info_q45.png
48;True or False: Models (3) and (4) will always have the same likelihood on D;True|False;False;False. Model (3) and (4) imply different independence assumptions (different I-maps). Therefore, they will fit the data differently and produce different likelihood scores.;info_q45.png
49;True or False: Model (1) can never have a higher likelihood than Model (2) on D;True|False;True;True. Model (2) is a **superset** of Model (1). Any distribution M1 can represent, M2 can also represent (by setting edge weights to 0). Therefore, M2 can always fit the data at least as well as M1, usually better. (Likelihood always increases with complexity, only Scores like BIC penalize it).;info_q45.png
50;True or False: Model (2) is preferable to (4) because it models causal direction;True|False;False;False. Standard Structure Learning scores (like Likelihood or BIC) are "Markov Equivalent". They care about correlation, not causality. They cannot distinguish between $A \rightarrow B$ and $A \leftarrow B$ if the independencies are the same.;info_q45.png
51;If all four models had the same likelihood, BIC would prefer model:;1|2|3|4;1;Model 1 is the simplest (fewest edges, fewest parameters). If Likelihoods are equal, the BIC formula (Likelihood - Penalty) is maximized by minimizing the Penalty. Model 1 has the smallest penalty.;info_q45.png
52;HMM: Give the dimension of Matrix B;rows: N / columns: M|rows: M / columns: N;rows: N / columns: M;Matrix B is the **Observation Model** $P(Observation | State)$. It maps $N$ possible states to $M$ possible observations. So it is size $N \times M$. (Rows sum to 1).;info_q52.png
53;HMM: Give the dimension of $P(o_2|S)$;numbers: M|numbers: N;numbers: M;This asks for the probability of the observation $o_2$ given the states. Since the variable is the observation (which has M possibilities), the distribution has size M.;info_q52.png
54;HMM: Give the dimension of $P(S|o_1)$;numbers: N|numbers: M;numbers: N;This is the "filtered" or "smoothed" estimate of the State. Since there are $N$ states, the probability vector has size $N$. (e.g., P(Sunny), P(Rainy)).;info_q52.png
55;HMM: Give the dimension of A (Transition);rows: N / columns: N|rows: N / columns: M;rows: N / columns: N;Matrix A is the **Transition Model** $P(State_t | State_{t-1})$. It maps from State to State. Since both are size $N$, the matrix is square $N \times N$. (Rows sum to 1).;info_q52.png
56;HMM: Give the dimension of $P(O|s_N)$;numbers: M|numbers: N;numbers: M;This is a row from Matrix B. Given a specific state $s_N$, what is the probability of seeing Observation $O$? Since there are $M$ possible observations, the vector is size $M$.;info_q52.png
57;HMM: Give the dimension of $\Pi$ (Initial);rows: N / columns: 1|rows: 1 / columns: N;rows: N / columns: 1;$\Pi$ is the initial state distribution $P(S_0)$. Since there are $N$ states, it is a vector of size $N \times 1$. (or $1 \times N$ depending on notation, but size is N).;info_q52.png
58;True or False: HMM2 Topological order: S(0), S(1), O(1), S(2), O(2);True|False;True;True. Topological sort means parents come before children. $S_0 \rightarrow S_1 \rightarrow S_2$. And $S_t \rightarrow O_t$. This order respects all arrows: $S_0$ is before $S_1$. $S_1$ is before $O_1$ and $S_2$. This is a valid order.;info_q58.png
59;True or False: HMM2 Topological order: S(0:2), O(1:2);True|False;True;True. This means "All States first, then All Observations". Since observations are leaves (endpoints) in an HMM, listing them last is always valid.;info_q58.png
60;True or False: HMM2 Topological order: S(0), O(1), S(1), O(2), S(2);True|False;False;False. This puts $O_1$ before $S_1$. But in an HMM, the arrow goes $S_1 \rightarrow O_1$. You cannot have the child ($O_1$) appear before the parent ($S_1$). Invalid.;info_q58.png
61;True or False: $P(S^{(t+1)}|O)$ can be predicted by Filtering followed by Prediction;True|False;True;True. This is the standard "One-step-ahead Prediction". First, you Filter to get the current state estimate $P(S_t | O_{1:t})$. Then, you apply the Transition Matrix to Predict the next state $P(S_{t+1}|O_{1:t})$.(Filter -> Predict).;info_q61.png
62;True or False: Given $O$, one can ignore observations and still predict $P(S^{(t)})$;True|False;True;True. You *can* ignore observations. This just reduces to predicting the prior evolution of the system based only on the Transition Matrix. It won't be very accurate, but it is mathematically possible to compute.;info_q61.png
63;True or False: The message in prediction algorithm always sums to 1;True|False;True;True. The message represents a probability distribution $P(S)$. Distributions must sum to 1. In standard algorithms, we normalize at every step.;info_q61.png
64;True or False: Smoothing could be done by unrolling HMM and using Inference by Enumeration;True|False;True;True. An HMM is just a Bayesian Network. You *could* unroll it into a giant graph for T steps and run standard exact inference. It would be slow (exponential in T), but it is theoretically correct.;info_q61.png
65;True or False: Kalman Filter calculates $P(P^{(t)},V^{(t)}|z^{(1:t)})$;True|False;True;True. This is the exact definition of **Filtering** in a Kalman Filter: Estimating the current hidden state (Position, Velocity) given all observations up to the current time ($z_{1:t}$).;info_q65.png
66;True or False: Kalman Filter calculates $P(P^{(t)},V^{(t)}|z^{(t)},p^{(t-1)},v^{(t-1)})$;True|False;False;False. This formula conditions on the *previous state values* ($p, v$) exactly. But in Filtering, we don't know the previous state exactly, we only have a probability distribution (belief) over it. The Kalman Filter conditions on the *previous belief*, not the values themselves.;info_q65.png
67;True or False: Kalman Filter calculates $P(P^{(t)},V^{(t)}|P^{(t-1)},V^{(t-1)})$;True|False;False;False. This is just the Transition Model (Physics), not the Filter. It ignores observations.;info_q65.png
68;True or False: Kalman Filter calculates $P(P^{(t)},V^{(t)}|z^{(1:t)},p^{(t-1)},v^{(t-1)})$;True|False;False;False. Redundant conditioning. If we knew the previous state exactly, we wouldn't need $z_{1:t-1}$. If we don't, we can't condition on $p, v$. Incorrect notation.;info_q65.png
69;True or False: Kalman Filter calculates $P(P^{(t)},V^{(t)}|z^{(t)})$;True|False;False;False. This uses only the *current* observation. It ignores all history. This is not Filtering, it's just local estimation.;info_q65.png
70;True or False: Assumption: GPS reading and velocity at same time are independent;True|False;False;False. In the model, Velocity affects Position. Position affects GPS Reading. Therefore, Velocity and GPS are correlated. They are not independent.;info_q70.png
71;True or False: Assumption: Prob of slowing down is independent of current velocity;True|False;True;True. The standard Linear Gaussian model usually assumes $V_t = V_{t-1} + \text{noise}$. The noise (acceleration/deceleration) is drawn from a fixed Gaussian $N(0, \Sigma)$, which is independent of the value of $V$. (The noise doesn't "know" how fast you are going).;info_q70.png
72;True or False: Assumption: GPS sensor has same inaccuracy everywhere;True|False;True;True. The Observation Noise covariance matrix $R$ is usually constant (Stationary). It does not change based on where you are on the map.;info_q70.png
73;True or False: Assumption: Distribution of GPS values depends only on $p^{(t)}$;True|False;True;True. The observation model is $Z_t = H \cdot P_t + \text{noise}$. The GPS reading depends directly only on the current Position, not on Velocity or past history (Markov Property).;info_q70.png
74;True or False: Assumption: Velocity tends to stay same from moment to moment;True|False;True;True. This is the **Inertia** assumption encoded in the transition matrix: $V_t \approx V_{t-1}$. Without this, the object would jitter randomly like Brownian motion.;info_q70.png
75;True or False: Assumption: GPS readings give no information about velocity;True|False;False;False. While a *single* GPS reading tells you nothing about velocity, a *sequence* of GPS readings over time provides huge information about velocity (change in position / time). The Filter uses this to estimate V.;info_q70.png